{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "- Linear Regression is a types of supervised machine learning where we attempt to predict continuous variables given several independent variables.\n",
    "\n",
    "- **Goal:**\n",
    "    - To create a mathematical model that captures linear relationship between dependent and independent variables.\n",
    "    - To generate predictions.\n",
    "    - To assist in Decision Making by quantifying the expected outcomes based on changes in independent/predictors variables.\n",
    "\n",
    "- \n",
    "- **Regression Types**:\n",
    "    1. `Simple Linear Regression`\n",
    "    2. `Multiple Linear Regresssion`\n",
    "    3. `Simple Polynomial Regression`\n",
    "    4. `Multiple Polynomial Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple Linear Regression\n",
    "\n",
    "- In Simple Linear Regression, we consider a single independent variable and a single dependent variable.\n",
    "- It helps to figure out the relationship between 2 variables i.e. independent variable (say x) and dependent variable (say y).\n",
    "- Mathematical model of simple linear regression takes the form of straight line.\n",
    "\n",
    "- **Mathematically,**\n",
    "    - `Y = β0 + β1X` \n",
    "        - β0: the intercept\n",
    "        - β1: the slope\n",
    "        - X: an independent variable (the variable used to predict Y)\n",
    "        - Y: dependent variable (the variable we want to predict)\n",
    "- Here the term β0, β1 are called model parameters which will be estimated using optimization techniques such as Gradient Descent via minimizing the objective or cost function.\n",
    "\n",
    "- **Visually (β0=38423, β1=821)**\n",
    "    - `Y = 38423 + 821X`\n",
    "    - <img src='images/1.png' width='400'>\n",
    "    - Here, we fit the Linear Regression Line on Scatter data.\n",
    "    - After, we can make prediction on New X via project to the line and looking to corresponding Y value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simple Linear Regression (Training Process Using Ordinary Least Squares (OLS))**\n",
    "- OLS stands for Ordinary Least Square.\n",
    "- OLS by default uses mean squared error.\n",
    "- Sum of Squared error i.e. SSE (β0 and β1)\n",
    "    - Σ(Yi - (β0 + β1 * Xi))² \n",
    "- Mean Squared Error  i.e. MSE (β0 and β1) \n",
    "    - SSE (β0 and β1) / N\n",
    "- OLS says using some formulae, It is possible to compute β0 and β1.\n",
    "- Aim: To select the best fit line (optimal β0 and β1) that reduces the error between predicted and actual value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of OLS Estimate for Single Linear Regression\n",
    "\n",
    "We start with the simple linear regression model:\n",
    "\n",
    "\n",
    "Y = β0 + β1*X\n",
    "\n",
    "\n",
    "Where:\n",
    "- Y is the observed dependent variable.\n",
    "- X  is the independent variable.\n",
    "- β0 is the intercept.\n",
    "- β1 is the slope.\n",
    "\n",
    "**Step 1: Define the Cost Function**\n",
    "\n",
    "The goal is to minimize the sum of squared errors (SSE), which is the sum of the squared differences between the observed Y and the predicted \n",
    "$(\\hat{Y})$ values:\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^{N} (Y_i - \\hat{Y}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "**Step 2: Minimize SSE by Finding $(\\beta_0)$**\n",
    "\n",
    "To minimize SSE, we differentiate it with respect to $( \\beta_0 )$ and set the derivative equal to 0 (Since Minimum error is the point where Derivative of Error function is 0).\n",
    "\n",
    "Differentiate SSE with respect to $(\\beta_1)$: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta_0} = \\frac{\\partial \\left( \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))^2 \\right)}{\\partial \\beta_1}\n",
    "$$\n",
    "\n",
    "Apply the chain rule and simplify:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta_0} = -2 \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))\n",
    "$$\n",
    "\n",
    "Set the derivative equal to 0 and solve for $(\\beta_0)$:  \n",
    "$$\n",
    "-2 \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i)) = 0  \n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i)) = 0\n",
    "$$\n",
    "\n",
    "Now, solving for $(\\beta_0)$:  \n",
    "\n",
    "$$\n",
    "\\beta_0 = \\bar{Y} - \\beta_1\\bar{X}          --> (i)\n",
    "$$\n",
    "\n",
    "\n",
    "**Step 3: Minimize SSE by Finding $(\\beta_1)$**\n",
    "\n",
    "To minimize SSE, we differentiate it with respect to $( \\beta_1 )$ and set the derivative equal to 0 (Since Minimum error is the point where Derivative of Error function is 0).\n",
    "\n",
    "Differentiate SSE with respect to $(\\beta_1)$: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta_1} = \\frac{\\partial \\left( \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))^2 \\right)}{\\partial \\beta_1}\n",
    "$$\n",
    "\n",
    "Apply the chain rule and simplify:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial SSE}{\\partial \\beta_1} = -2 \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))X_i\n",
    "$$\n",
    "\n",
    "Set the derivative equal to 0 and solve for $( \\beta_1 )$:\n",
    "\n",
    "$$\n",
    "-2 \\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))X_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N} (Y_i - (\\beta_0 + \\beta_1X_i))X_i = 0\n",
    "$$\n",
    "\n",
    "Solving for $( \\beta_1)$ by substituting value of $( \\beta_0)$:\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum_{i=1}^{N} (Y_i - \\bar{Y})}{\\sum_{i=1}^{N} X_i - \\bar{X}} \n",
    "$$\n",
    "\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum_{i=1}^{N} (Y_i - \\bar{Y})}{\\sum_{i=1}^{N} X_i - \\bar{X}} \n",
    "$$  \n",
    "\n",
    "\n",
    "$$\n",
    "\\beta_0 = \\bar{Y} - \\beta_1\\bar{X}          \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression (using OLS)\n",
    "1. Define the Linear Model\n",
    "    - Y = β0 + β1*X  \n",
    "    \n",
    "    - Y is the observed dependent variable.\n",
    "    - X  is the independent variable.\n",
    "    - β0 is the intercept.\n",
    "    - β1 is the slope.\n",
    "\n",
    "2. Define the objective function\n",
    "    - Objective in Linear Regression is to find β0 and β1 that minimizes the sum of squared errors (SSE).\n",
    "    - The error for each data point is the difference between the observed value and the predicted value:\n",
    "    - `e(i) = Y(i) -  β0 + β1*X(i)`\n",
    "    - SSE = Σ(Yi - (β0 + β1 * Xi))²\n",
    "\n",
    "3. Minimize the Objective function\n",
    "    - As computed in the above section, To minimize SSE, we take the partial derivative of SSE with respect to β0 and β1, set them equal to zero, and solve for β0 and β1.\n",
    "    - Refer to above section for the derivation part.\n",
    "\n",
    "4. Final OLS Equation\n",
    "    - $\\hat{Y}$ = β0 + β1*X\n",
    "    - where, β0 and β1 is esimated using the formulae derived in the above section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simple Linear Regression (Training Process using Gradient Descent)**\n",
    ">Linear Regression using gradient descent is an optimization technique for finding the coefficient (slope and intercept) of a linear regression model that minimizes the cost function (typically MSE or Mean Squared Error).\n",
    "\n",
    "**Step 1:** Initialize the model parameters randomly.\n",
    "- Initialize the intercept (β₀) and slope (β₁) with random values.\n",
    "\n",
    "**Step 2:** Compute the gradients of the cost function (MSE) with respect to the parameters β₀ and β₁.\n",
    "- `Mean Squared Error (MSE) Formula`\n",
    "  - MSE is a measure of the average squared difference between predicted and actual values in the dataset and is calculated as follows:\n",
    "    - MSE = (1/N) * Σ(yᵢ - (β₀ + β₁ * xᵢ))² for i = 1 to N, where N is the number of data points.\n",
    "\n",
    "\n",
    "- Calculate the gradients using partial derivatives:\n",
    "  - ∂MSE/∂β₀ = -2/N * Σ(yᵢ - (β₀ + β₁ * xᵢ))\n",
    "  - ∂MSE/∂β₁ = -2/N * Σxᵢ * (yᵢ - (β₀ + β₁ * xᵢ)), where N is the number of data points.\n",
    "\n",
    "**Step 3:** Update the parameters of the model by taking steps in the opposite direction of the gradients.\n",
    "- Update the intercept (β₀) and slope (β₁) using the learning rate α (alpha):\n",
    "  - β₀ = β₀ - α * ∂MSE/∂β₀\n",
    "  - β₁ = β₁ - α * ∂MSE/∂β₁\n",
    "\n",
    "**Step 4:** Repeat steps 2 and 3 iteratively until convergence.\n",
    "- Iterate through steps 2 and 3 until a stopping criterion is met (e.g., a maximum number of iterations or a small change in MSE). The goal is to find the optimal values of β₀ and β₁ that minimize the MSE and provide the best fit for the simple linear regression model.\n",
    "\n",
    "In summary, the gradient descent algorithm for simple linear regression aims to find the optimal intercept (β₀) and slope (β₁) by iteratively adjusting these parameters in the direction that minimizes the mean squared error (MSE) between the predicted and actual values of the target variable. This process continues until convergence is achieved or a predefined stopping condition is met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multiple Linear Regression\n",
    "- Unlike Single Linear Regression, In multiple linear regression there will be a multiple independent variables i.e. x1, x2, x3, and so on.\n",
    "- The term Linear means it will be line in 2D, Plane in 3D, and Hyperplane in higher dimensions.\n",
    "- The equation of Multiple Linear Regression is given by,\n",
    "$$\n",
    "Yhat = a + b_1 X_1 + b_2 X_2 + b_3 X_3 + b_4 X_4\n",
    "$$\n",
    "\n",
    "$$\n",
    "a: intercept\\\\\\\\\n",
    "b_1 :coefficients \\ of\\ Variable \\ 1\\\\\\\\\n",
    "b_2: coefficients \\ of\\ Variable \\ 2\\\\\\\\\n",
    "b_3: coefficients \\ of\\ Variable \\ 3\\\\\\\\\n",
    "b_4: coefficients \\ of\\ Variable \\ 4\\\\\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "Yhat: Response \\ Variable\\\\\\\\\n",
    "X_1 :Predictor\\ Variable \\ 1\\\\\\\\\n",
    "X_2: Predictor\\ Variable \\ 2\\\\\\\\\n",
    "X_3: Predictor\\ Variable \\ 3\\\\\\\\\n",
    "X_4: Predictor\\ Variable \\ 4\\\\\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "- The algorithm discussed in simple linear regression to get optimal parameters can also be applied to multiple linear regression. The only difference is we now have more independent variables meaning more parameters. But the idea remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics (Regression Models)\n",
    "- Useful in explaning the performance of the model.\n",
    "- Based on actual value, we can compare how well our model is performing, via comparing predicted value with actual value.\n",
    "\n",
    "- **Error:**\n",
    "    - Error of Regression model is a measure of how far the data is from the fitted regression line.\n",
    "    \n",
    "- **Different evaluation metrics are:**\n",
    "    - `Mean Absolute Error (MAE)`\n",
    "    - `Mean Square Error (MSE)`\n",
    "    - `Root Mean Square Error (RMSE)`\n",
    "    - `Coefficient of Determination (R^2)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Mean Absolute Error (MAE)**\n",
    "- Mean absolute error is the mean of the absolute value of the errors.\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(n\\) is the number of data points.\n",
    "- $y_i$ is the actual value of the target variable for the i-th data point.\n",
    "- $\\hat{y}_i$ is the predicted value for the i-th data point.\n",
    "\n",
    "\n",
    "\n",
    "`Example:`  \n",
    "Suppose we have a dataset with three data points, and we want to calculate the MAE for a simple linear regression model:\n",
    "\n",
    "- Actual Values ($y$): [2, 4, 7]  \n",
    "- Predicted values ($y_i$): [2.5, 3.8, 6.2]  \n",
    "\n",
    "Using the formulae:  \n",
    "$$\n",
    "MAE = \\frac{1}{3} \\left(|2 - 2.5| + |4 - 3.8| + |7 - 6.2|\\right) = \\frac{1}{3} \\cdot 0.3 = 0.1\n",
    "$$\n",
    "\n",
    "\n",
    "- Here absolute value is used otherwise positive and negative number cancel each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Mean Squared Error (MSE)**  \n",
    "- Mean squared error is the mean of the squared error.  \n",
    "\n",
    "Mathematical Notation:\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(n\\) is the number of data points.\n",
    "- $y_i$ is the actual value of the target variable for the i-th data point.\n",
    "- $\\hat{y}_i$ is the predicted value for the i-th data point.\n",
    "\n",
    "`Example:`\n",
    "Using the same dataset as before:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{3} \\left((2 - 2.5)^2 + (4 - 3.8)^2 + (7 - 6.2)^2\\right) = \\frac{1}{3} \\cdot 0.09 = 0.03\n",
    "$$\n",
    "\n",
    "So, the MSE for this model is 0.03.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Root mean squared error**\n",
    "- Root mean squared error is the square root of the mean squared error.\n",
    "\n",
    "Mathematical Notation:\n",
    "$$\n",
    "RMSE = \\sqrt{MSE}\n",
    "$$\n",
    "\n",
    "\n",
    "**Example:**\n",
    "Continuing from the previous example:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{0.03} \\approx 0.1732\n",
    "$$\n",
    "\n",
    "So, the RMSE for this model is approximately 0.1732.\n",
    "\n",
    "In summary, these evaluation metrics help us assess the accuracy of regression models, with lower values indicating better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Coefficient of Determination (R^2)**\n",
    "- $R^2$ is also called the Coefficient of Determination.\n",
    "- It's a measure to determine how close the data is to the fitted regression line.\n",
    "- It takes value in between 0 and 1. \n",
    "    - 0 indicates poor model, and 1 indicates high model.\n",
    "\n",
    "The formula for the coefficient of determination $R^2$ in linear regression is given by:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SSR}{SST}$$\n",
    "\n",
    "Where:\n",
    "- $R^2$ is the coefficient of determination (R-squared).\n",
    "- $SSR$ is the sum of squared residuals (also known as the sum of squared errors or SSE), which represents the total unexplained variation in the dependent variable by the model.\n",
    "    - The formula for SSR is:\n",
    "\n",
    "    $$SSR = \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "    Where:  \n",
    "    - $n$ is the number of data points.\n",
    "    - $(\\hat{y}_i)$ is the predicted value for the i-th data point.\n",
    "    - $y_i$ is the actual value of the dependent variable for the i-th data point.\n",
    "   \n",
    "- \\(SST\\) is the total sum of squares, which represents the total variation in the dependent variable.\n",
    "    - The formula for SST is:\n",
    "\n",
    "  $$SST = \\sum_{i=1}^{n} (y_i - \\bar{y})^2$$\n",
    "\n",
    "  Where:  \n",
    "   - $n$ is the number of data points.\n",
    "   - $y_i$ is the actual value of the dependent variable for the i-th data point.\n",
    "   - $(\\bar{y})$ is the mean (average) of the actual values $(y_i)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "- https://www.youtube.com/watch?v=KZ1mWboXE6g\n",
    "- https://www.coursera.org/learn/data-analysis-with-python/lecture/Wlyce/linear-regression-and-multiple-linear-regression\n",
    "- https://www.geeksforgeeks.org/ml-linear-regression/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
